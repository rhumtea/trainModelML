{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+q5v2Ro6Xy69hmtUsHxFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhumtea/trainModelML/blob/main/phong_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction:\n",
        "- This project designs and train the Deep Learnign model.\n",
        "- I choose the Natural Inmages with 8 classes from Kaggle.\n",
        "- Link: https://www.kaggle.com/datasets/prasunroy/natural-images"
      ],
      "metadata": {
        "id": "JXPxPCQB_KOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset from Kaggle and save in Google Colab Files\n",
        "!kaggle datasets download -d prasunroy/natural-images"
      ],
      "metadata": {
        "id": "oIbGDKjSBNHq",
        "outputId": "2b1a975b-e1f0-4df9-d8cf-d5c986526d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/prasunroy/natural-images\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading natural-images.zip to /content\n",
            " 99% 340M/342M [00:04<00:00, 120MB/s]\n",
            "100% 342M/342M [00:04<00:00, 85.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the natural-images.zip\n",
        "with zipfile.ZipFile('natural-images.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('')\n",
        "\n",
        "# Define path for original dataset\n",
        "data_dir = 'natural_images'"
      ],
      "metadata": {
        "id": "4VHREdU-ELm-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define path for split data\n",
        "base_dir = 'data_split'\n",
        "\n",
        "# Define path for train, validation and test directory\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "VCAhObhXEu2m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split ratio\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15"
      ],
      "metadata": {
        "id": "Yq0nNSgQGwNw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "categories = os.listdir(data_dir)\n",
        "\n",
        "for category in categories:\n",
        "  category_path = os.path.join(data_dir, category)\n",
        "\n",
        "  if not os.path.isdir(category_path): continue\n",
        "\n",
        "  # Create subdirectories for each class in train and val files\n",
        "  os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "  os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "\n",
        "  # Get all images in the category to split\n",
        "  images = os.listdir(category_path)\n",
        "  random.shuffle(images)\n",
        "\n",
        "  # Find indices of image to split\n",
        "  total_images = len(images)\n",
        "  train_images_index = int(train_ratio * total_images)\n",
        "  val_images_index = int((train_ratio + val_ratio) * total_images)\n",
        "\n",
        "  # Split data by indices\n",
        "  train_images = images[:train_images_index]\n",
        "  val_images = images[train_images_index:val_images_index]\n",
        "  test_images = images[val_images_index:]\n",
        "\n",
        "  # Move images to train and val directories with folders as original file:\n",
        "  for image in train_images:\n",
        "    shutil.copy(os.path.join(category_path, image), os.path.join(train_dir, category))\n",
        "\n",
        "  for image in val_images:\n",
        "    shutil.copy(os.path.join(category_path, image), os.path.join(val_dir, category))\n",
        "\n",
        "  # Move all images to test directory:\n",
        "  for image in test_images:\n",
        "    shutil.copy(os.path.join(category_path, image), test_dir)"
      ],
      "metadata": {
        "id": "01mYUmCYHALo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define train generator which includes augmentation to expand the dataset\n",
        "# and make the model more roburst\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range = 30,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        ")"
      ],
      "metadata": {
        "id": "OewU47BxK326"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define validation and test generators with normalization\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)"
      ],
      "metadata": {
        "id": "I_icj8THMJdm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image height, width for resizing image from th original size (uniformity)\n",
        "# Set batch_size for deciding how many images per batch during train and validation\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "v3myl-HIOqXb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical' # because of one-hot encode labels\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "phkA_RNHPitp",
        "outputId": "5369e3dc-9434-4dc1-fcf2-7a53b2ceb215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4826 images belonging to 8 classes.\n",
            "Found 1034 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generator should be unlabeled.\n",
        "import pandas as pd\n",
        "\n",
        "# Make a list of all test images\n",
        "test_images = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)]\n",
        "print(len(test_images))\n",
        "\n",
        "# Convert test_images into DataFrame - contains file paths\n",
        "test_df = pd.DataFrame({'testimages' : test_images})\n",
        "\n",
        "# Test generator\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe = test_df,\n",
        "    x_col = 'testimages',\n",
        "    y_col = None,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = None,\n",
        "    shuffle = False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "i00cZHASUMF0",
        "outputId": "ca11b869-0d82-4f44-e1b6-5b4eb74f011b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1039\n",
            "Found 1039 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Check before create Model"
      ],
      "metadata": {
        "id": "X2Te7G3aadQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of classes\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(num_classes)\n",
        "class_names = [item for item in train_generator.class_indices]\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "zQR5SAPPZ2OL",
        "outputId": "214ccb89-d0cb-4b7b-ddcb-4fd2bde0d70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Pre-Trained ResNet50 Model"
      ],
      "metadata": {
        "id": "UFEL5Gr8IW-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load  the Pre-Trained ResNet50 model\n",
        "pretrained_model = ResNet50(weights='imagenet', pooling='avg', include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "metadata": {
        "id": "dzSTZ_yaGASi",
        "outputId": "8324246b-46f7-4de0-b4fe-18ffbb0c6879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the pretrained model\n",
        "pretrained_model.trainable = True\n",
        "\n",
        "# Freeze first 140 layers of the pretrained model\n",
        "for layer in pretrained_model.layers[:140]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "VwHJLryhHMjs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model for natural images"
      ],
      "metadata": {
        "id": "scPuz1eXRPMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom layer for natural image dataset\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    pretrained_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "rOFaD6SnHesB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FvgIN_1EJ5Bu",
        "outputId": "55c37199-7baa-4a36-ba84-ab0c5e4cff0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │         \u001b[38;5;34m131,136\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m520\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,719,368\u001b[0m (90.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,719,368</span> (90.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m131,656\u001b[0m (514.28 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,656</span> (514.28 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "IrYGvnNYIMBY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data = val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "GJ44GwbpJ1U2",
        "outputId": "71605718-4c82-4279-f2fd-3d9158b1f936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1643s\u001b[0m 11s/step - accuracy: 0.4590 - loss: 1.6219 - val_accuracy: 0.4613 - val_loss: 1.6744\n",
            "Epoch 2/50\n",
            "\u001b[1m 12/151\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:53\u001b[0m 9s/step - accuracy: 0.6184 - loss: 1.2105 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Save the Model --\n"
      ],
      "metadata": {
        "id": "bu1cmqmPpBHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and Reuse Model\n",
        "model.save('resnet50_model.keras')"
      ],
      "metadata": {
        "id": "J9synASXo_fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Load the Model --"
      ],
      "metadata": {
        "id": "U7W0JwripRtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('resnet50_model.keras')"
      ],
      "metadata": {
        "id": "F_RlapXLpfYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate Model"
      ],
      "metadata": {
        "id": "HspqPs9uoHqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "u5WjXZbXoEE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}